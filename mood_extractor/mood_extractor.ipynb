{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yeEVxAZHI-9U"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torch import nn, optim\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN6zYcvwJAbr"},"outputs":[],"source":["df = pd.read_csv(\"training.csv\")\n","\n","label_encoder = LabelEncoder()\n","df['label'] = label_encoder.fit_transform(df['label'])\n","\n","# Split dataset and reset index\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","train_df = train_df.reset_index(drop=True)\n","val_df = val_df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Idb3rtlqOfWg"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","MAX_LEN = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYYaGUu_L11I"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","class TextDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.text\n","        self.targets = dataframe.label\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        target = self.targets[index]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'text': text,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'targets': torch.tensor(target, dtype=torch.long)\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDn-VeY6OjcL"},"outputs":[],"source":["train_dataset = TextDataset(train_df, tokenizer, MAX_LEN)\n","val_dataset = TextDataset(val_df, tokenizer, MAX_LEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9usXVdZOo1S"},"outputs":[],"source":["BATCH_SIZE = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4aVzXPXOrc2"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdyfHpHJOyVC"},"outputs":[],"source":["class CNNModel(nn.Module):\n","  def __init__(self, vocab_size, embed_size, num_classes):\n","    super(CNNModel, self).__init__()\n","    self.embedding = nn.Embedding(vocab_size, embed_size)\n","    self.conv_layer_1 = nn.Conv2d(1, 100, (3, embed_size))\n","    self.conv_layer_2 = nn.Conv2d(1, 100, (4, embed_size))\n","    self.conv_layer_3 = nn.Conv2d(1, 100, (5, embed_size))\n","    self.dropout = nn.Dropout(0.5)\n","    self.fc = nn.Linear(300, num_classes)\n","\n","  def forward(self, x):\n","    x = self.embedding(x).unsqueeze(1)\n","\n","    x1 = torch.relu(self.conv_layer_1(x)).squeeze(3)\n","    x1 = torch.max_pool1d(x1, x1.size(2)).squeeze(2)\n","\n","    x2 = torch.relu(self.conv_layer_2(x)).squeeze(3)\n","    x2 = torch.max_pool1d(x2, x2.size(2)).squeeze(2)\n","\n","    x3 = torch.relu(self.conv_layer_3(x)).squeeze(3)\n","    x3 = torch.max_pool1d(x3, x3.size(2)).squeeze(2)\n","\n","    x = torch.cat((x1, x2, x3), 1)\n","    x = self.dropout(x)\n","\n","    return self.fc(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVVxtJdsO1u_"},"outputs":[],"source":["VOCAB_SIZE = len(tokenizer.vocab)\n","EMBED_SIZE = 128\n","NUM_CLASSES = len(label_encoder.classes_)\n","\n","model = CNNModel(VOCAB_SIZE, EMBED_SIZE, NUM_CLASSES)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXsFJnKGRFqR"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WeS5y4VkRaKB"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(), lr=0.001)\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFqhf3SDRkMe"},"outputs":[],"source":["def train_model(model, data_loader, loss_fn, optimizer, device):\n","  model.train()\n","  losses = []\n","  correct_predictions = 0\n","\n","  for data in data_loader:\n","    input_ids = data[\"input_ids\"].to(device)\n","    targets = data[\"targets\"].to(device)\n","\n","    outputs = model(input_ids)\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, targets)\n","\n","    correct_predictions += torch.sum(preds == targets)\n","    losses.append(loss.item())\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","  return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device):\n","  model.eval()\n","  losses = []\n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for data in data_loader:\n","      input_ids = data[\"input_ids\"].to(device)\n","      targets = data[\"targets\"].to(device)\n","\n","      outputs = model(input_ids)\n","      _, preds = torch.max(outputs, dim=1)\n","      loss = loss_fn(outputs, targets)\n","\n","      correct_predictions += torch.sum(preds == targets)\n","      losses.append(loss.item())\n","  return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189080,"status":"ok","timestamp":1722186731217,"user":{"displayName":"Şerif Ağır","userId":"09014416936363875135"},"user_tz":-180},"id":"uR1OBhYQSdKK","outputId":"af51f6d8-1d3f-4a37-ed7f-4de022d89c5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","----------\n","Train loss 1.5799855598807335 accuracy 0.37421875\n","Validation loss 1.3227023911476135 accuracy 0.506875\n","Epoch 2/10\n","----------\n","Train loss 1.0811497980356217 accuracy 0.60265625\n","Validation loss 0.7099590268731117 accuracy 0.7528125\n","Epoch 3/10\n","----------\n","Train loss 0.5713352465629578 accuracy 0.800546875\n","Validation loss 0.40883268252015115 accuracy 0.8553125\n","Epoch 4/10\n","----------\n","Train loss 0.3266574245411903 accuracy 0.884375\n","Validation loss 0.3421043717861176 accuracy 0.8709375\n","Epoch 5/10\n","----------\n","Train loss 0.22097457921598107 accuracy 0.923671875\n","Validation loss 0.3101786329969764 accuracy 0.885\n","Epoch 6/10\n","----------\n","Train loss 0.15426871123723684 accuracy 0.947578125\n","Validation loss 0.3148319625854492 accuracy 0.8865625\n","Epoch 7/10\n","----------\n","Train loss 0.11865340289194137 accuracy 0.9606250000000001\n","Validation loss 0.3204065935313702 accuracy 0.8928125\n","Epoch 8/10\n","----------\n","Train loss 0.0960784015111858 accuracy 0.9673437500000001\n","Validation loss 0.3385025025159121 accuracy 0.8859375\n","Epoch 9/10\n","----------\n","Train loss 0.07987000241875648 accuracy 0.97484375\n","Validation loss 0.3323202136904001 accuracy 0.89125\n","Epoch 10/10\n","----------\n","Train loss 0.07315313088460243 accuracy 0.97609375\n","Validation loss 0.3763127898424864 accuracy 0.8925000000000001\n"]}],"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","   print(f'Epoch {epoch + 1}/{EPOCHS}')\n","   print('-' * 10)\n","\n","   train_acc, train_loss = train_model(model, train_loader, loss_fn, optimizer, device)\n","   print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","   val_acc, val_loss = eval_model(model, val_loader, loss_fn, device)\n","   print(f'Validation loss {val_loss} accuracy {val_acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":390,"status":"ok","timestamp":1722186855834,"user":{"displayName":"Şerif Ağır","userId":"09014416936363875135"},"user_tz":-180},"id":"Dg5fNuc2TRpF","outputId":"f5616fd2-7a04-4086-dc39-c6e567eda727"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to mood_extractor_model.pkl\n"]}],"source":["model_path = 'mood_extractor_model.pkl'\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LoDmajipVRkV"},"outputs":[],"source":["class CNNModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_classes):\n","        super(CNNModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.conv1 = nn.Conv2d(1, 100, (3, embed_size))\n","        self.conv2 = nn.Conv2d(1, 100, (4, embed_size))\n","        self.conv3 = nn.Conv2d(1, 100, (5, embed_size))\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc = nn.Linear(300, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x).unsqueeze(1)\n","        x1 = torch.relu(self.conv1(x)).squeeze(3)\n","        x1 = torch.max_pool1d(x1, x1.size(2)).squeeze(2)\n","\n","        x2 = torch.relu(self.conv2(x)).squeeze(3)\n","        x2 = torch.max_pool1d(x2, x2.size(2)).squeeze(2)\n","\n","        x3 = torch.relu(self.conv3(x)).squeeze(3)\n","        x3 = torch.max_pool1d(x3, x3.size(2)).squeeze(2)\n","\n","        x = torch.cat((x1, x2, x3), 1)\n","        x = self.dropout(x)\n","        return self.fc(x)\n","\n","\n","# Load the tokenizer\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Define model parameters (should be same as during training)\n","VOCAB_SIZE = len(tokenizer.vocab)\n","EMBED_SIZE = 128\n","NUM_CLASSES = 6  # Assuming you have 6 emotion classes\n","\n","# Initialize the model\n","model = CNNModel(VOCAB_SIZE, EMBED_SIZE, NUM_CLASSES)\n","\n","# Load the model's state dictionary\n","model.load_state_dict(torch.load('mood_extractor_model.pkl'))\n","model.eval()  # Set the model to evaluation mode\n","\n","MAX_LEN = 128\n","\n","def preprocess_text(text, tokenizer, max_len):\n","    encoding = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=max_len,\n","        return_token_type_ids=False,\n","        padding='max_length',\n","        return_attention_mask=True,\n","        return_tensors='pt',\n","    )\n","    return encoding['input_ids'], encoding['attention_mask']\n","\n","# Example input text\n","input_text = \"I am feeling very happy today!\"\n","\n","input_ids, attention_mask = preprocess_text(input_text, tokenizer, MAX_LEN)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPbgy64ToKFi7Qf1a/WFBll"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}